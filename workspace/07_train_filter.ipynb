{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train filter\n",
    "\n",
    "スレタイの尤もらしさをスコアリングするFilterを学習する。\n",
    "\n",
    "FilterはCNNによる文書分類器、詳細は[scripts/model.py](scripts/model.py)を参照\n",
    "\n",
    "\n",
    "|パラメータ名|内容|\n",
    "|:--|:--|\n",
    "| `conv_filters` | Filterのパラメータ、詳細は[scripts/model.py](scripts/model.py)を参照 |\n",
    "| `conv_kernel_sizes` | Filterのパラメータ、詳細は[scripts/model.py](scripts/model.py)を参照 |\n",
    "| `d_model` | Filterのパラメータ、詳細は[scripts/model.py](scripts/model.py)を参照 |\n",
    "| `EPOCHS` | 学習のエポック数 |\n",
    "| `BATCH_SIZE` | 学習のバッチサイズ |\n",
    "\n",
    "## 入力ファイル\n",
    "\n",
    "* `real_dataset.pickle`\n",
    "* `fake_dataset.pickle`\n",
    "\n",
    "## 出力ファイル\n",
    "\n",
    "* `model/generator/weights_epoch*.h5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters \n",
    "conv_filters = [32, 64, 128]\n",
    "conv_kernel_sizes = [16, 8, 4]\n",
    "d_model = 128\n",
    "\n",
    "# 学習パラメータ\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('sentencepiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"real_dataset.pickle\", \"rb\") as f:\n",
    "    ids = pickle.load(f)\n",
    "pos_tensor = tf.keras.preprocessing.sequence.pad_sequences(ids, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fake_dataset.pickle\", \"rb\") as f:\n",
    "    neg_tensor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用: 学習データを減らす\n",
    "# pos_tensor = pos_tensor[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数を揃える\n",
    "neg_tensor = neg_tensor[:len(pos_tensor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536857 170762\n"
     ]
    }
   ],
   "source": [
    "pos_tensor_train, pos_tensor_valid = train_test_split(pos_tensor, test_size=0.1)\n",
    "print(len(pos_tensor_train), len(pos_tensor_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536857 170762\n"
     ]
    }
   ],
   "source": [
    "neg_tensor_train, neg_tensor_valid = train_test_split(neg_tensor, test_size=0.1)\n",
    "print(len(neg_tensor_train), len(neg_tensor_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pos_train = tf.data.Dataset.from_tensor_slices((pos_tensor_train, [1] * len(pos_tensor_train)))\n",
    "dataset_neg_train = tf.data.Dataset.from_tensor_slices((neg_tensor_train, [0] * len(neg_tensor_train)))\n",
    "dataset_pos_valid = tf.data.Dataset.from_tensor_slices((pos_tensor_valid, [1] * len(pos_tensor_valid)))\n",
    "dataset_neg_valid = tf.data.Dataset.from_tensor_slices((neg_tensor_valid, [0] * len(neg_tensor_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(pos_tensor_train) + len(neg_tensor_train)\n",
    "dataset_train = dataset_pos_train.concatenate(dataset_neg_train).shuffle(BUFFER_SIZE)\n",
    "BUFFER_SIZE = len(pos_tensor_valid) + len(neg_tensor_valid)\n",
    "dataset_valid = dataset_pos_valid.concatenate(dataset_neg_valid).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "vocab_size = sp.get_piece_size()\n",
    "seq_len = pos_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model import Filter\n",
    "nanj_filter = Filter(conv_filters, conv_kernel_sizes, d_model, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認（正例）\n",
    "test_case_pos = tf.constant(pos_tensor_valid[:10])\n",
    "scores = tf.math.sigmoid(nanj_filter(test_case_pos))\n",
    "for ids, score in zip(test_case_pos.numpy(), scores):\n",
    "    ids_int = list(map(lambda x: int(x), ids))\n",
    "    print(sp.decode_ids(ids_int))\n",
    "    print(f\"score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認（負例）\n",
    "test_case_neg = tf.constant(neg_tensor_valid[:10])\n",
    "scores = tf.math.sigmoid(nanj_filter(test_case_neg))\n",
    "for ids, score in zip(test_case_neg.numpy(), scores):\n",
    "    ids_int = list(map(lambda x: int(x), ids))\n",
    "    print(sp.decode_ids(ids_int))\n",
    "    print(f\"score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train = (len(pos_tensor_train) + len(neg_tensor_train))//BATCH_SIZE\n",
    "steps_per_epoch_valid = (len(pos_tensor_valid) + len(neg_tensor_valid))//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def filter_loss(real, pred):\n",
    "    loss = binary_cross_entropy(real, pred)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, label):    # x: (BATCH_SIZE, seq_len)\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = nanj_filter(inp, training=True)        \n",
    "        loss = filter_loss(label, pred)\n",
    "        \n",
    "    gradients = tape.gradient(loss, nanj_filter.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, nanj_filter.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(inp, label):    # x: (BATCH_SIZE, seq_len)    \n",
    "    pred = nanj_filter(inp, training=False)\n",
    "    loss = filter_loss(label, pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_valid = dataset_valid.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model/filter\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # TRAIN\n",
    "    total_loss = 0\n",
    "    for (batch, dataset) in enumerate(dataset_train.take(steps_per_epoch_train)):\n",
    "        inp, label = dataset\n",
    "        # batch_start = time.time()\n",
    "        batch_loss = train_step(inp, label)\n",
    "        total_loss += batch_loss\n",
    "        # print('Time taken for 1 batch {} sec'.format(time.time() - batch_start))\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "            \n",
    "    nanj_filter.save_weights(f\"{model_dir}/weights_epoch{epoch+1}.h5\")\n",
    "\n",
    "    print(f'Train Epoch {epoch+1} Gen Loss {total_loss/steps_per_epoch_train:.4f}')\n",
    "    \n",
    "    # VALIDATION\n",
    "    total_valid_loss = 0\n",
    "    for (batch, dataset) in enumerate(dataset_valid.take(steps_per_epoch_valid)):\n",
    "        inp, label = dataset\n",
    "        batch_loss = valid_step(inp, label)\n",
    "        total_valid_loss += batch_loss\n",
    "        \n",
    "    print(f'Validation Loss {total_valid_loss/steps_per_epoch_valid:.4f}')\n",
    "\n",
    "    # スコア付けを確認する\n",
    "    print(\">>> pos test case <<<\")\n",
    "    scores = tf.math.sigmoid(nanj_filter(test_case_pos))\n",
    "    for ids, score in zip(test_case_pos.numpy(), scores):\n",
    "        ids_int = list(map(lambda x: int(x), ids))\n",
    "        print(sp.decode_ids(ids_int))\n",
    "        print(f\"score: {score}\")\n",
    "\n",
    "    print(\">>> neg test case <<<\")\n",
    "    scores = tf.math.sigmoid(nanj_filter(test_case_neg))\n",
    "    for ids, score in zip(test_case_neg.numpy(), scores):\n",
    "        ids_int = list(map(lambda x: int(x), ids))\n",
    "        print(sp.decode_ids(ids_int))\n",
    "        print(f\"score: {score}\")\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
