{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess_threads\n",
    "\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦ï¼‘ãƒ•ã‚¡ã‚¤ãƒ«ã«çºã‚ã‚‹\n",
    "\n",
    "|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å|å†…å®¹|\n",
    "|:--|:--|\n",
    "| `THREAD_FILTER_TH_MIN_RESPONSE` | å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã®æœ€å°ãƒ¬ã‚¹æ•° |\n",
    "| `THREAD_FILTER_NG_PHRASES` | ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã®NGæ–‡å­—åˆ—ã€ã“ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã¯å­¦ç¿’ã«åˆ©ç”¨ã—ãªã„ã€‚ |\n",
    "| `IGNORE_PHRASES` | ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é™¤å¤–ã™ã‚‹æ–‡å­—åˆ— |\n",
    "| `SPECIAL_TOKEN_MAP` | å‰å‡¦ç†ã§å¤‰æ›ã™ã‚‹ã®ç‰¹æ®Šæ–‡å­— |\n",
    "| `SPECIAL_TOKEN_REGEX_MAP` | å‰å‡¦ç†ã§å¤‰æ›ã™ã‚‹ã®ç‰¹æ®Šæ–‡å­—ï¼ˆæ­£è¦è¡¨ç¾ï¼‰ |\n",
    "\n",
    "## å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "\n",
    "* `data/*.tsv`\n",
    "\n",
    "## å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "\n",
    "* `normalized_titles.txt`: æ­£è¦åŒ–æ¸ˆã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã€ï¼‘è¡Œã«1ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«\n",
    "* `normalizer.pickle`: æ­£è¦åŒ–ã‚’è¡Œã†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_FILTER_TH_MIN_RESPONSE = 5\n",
    "# ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¯lowercaseã«ã™ã‚‹\n",
    "THREAD_FILTER_NG_PHRASES = [\n",
    "    \"id:\", \"http\"\n",
    "]\n",
    "IGNORE_PHRASES = [\n",
    "    \"[ç„¡æ–­è»¢è¼‰ç¦æ­¢]Â©2ch.net\",\n",
    "    \"\\\\n\", \"\\n\", \"ãƒ»\", \"ã€\", \"ã€‚\", \",\", \".\",\n",
    "    \"â˜…\", \"â˜†\", \"â—‹\", \"â—\", \"â—\" \"â—¯ \", \"â– \", \"â–¡\"\n",
    "]\n",
    "SPECIAL_TOKEN_MAP = {\n",
    "    \"å½¡(ã‚š)(ã‚š)\": \"<nanjmin>\",\n",
    "    \"å½¡(^)(^)\": \"<nikojmin>\",\n",
    "    \"å½¡(;)(;)\": \"<nakijmin>\",\n",
    "    \"ï¼ˆãƒ½Â´ã‚“`ï¼‰\": \"<kenmoukun>\",\n",
    "    \"(ãƒ½ Ìã‚“`)\": \"<kenmoukun>\",\n",
    "    \"(ãƒ½â€™ã‚“`)\": \"<kenmoukun>\",\n",
    "    \"(ãƒ½'ã‚“`)\": \"<kenmoukun>\",\n",
    "    \"( ãƒ½ Ìã‚“`)\": \"<kenmoukun>\",\n",
    "    \"(Â´ãƒ»Ï‰ãƒ»`)\": \"<genjumin>\",\n",
    "    \"(Â´ãƒ»Ï‰ãƒ»ï½€)\": \"<genjumin>\",\n",
    "    \"( Ì;Ï‰;`)\": \"<nakigenjumin>\",\n",
    "    \"(*^â—¯^*)\": \"<pojihamekun>\",\n",
    "    \"(*^^*)\": \"<pojihamekun>\",\n",
    "    \"( ÌÏ‰`)\": \"<kao1>\",\n",
    "}\n",
    "SPECIAL_TOKEN_REGEX_MAP = {\n",
    "    re.compile(\"ww+\"): \"<kusa>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹\n",
    "titles = []\n",
    "for fpath in glob.glob(\"data/*.tsv\"):\n",
    "    with open(fpath) as f:\n",
    "        for line in f:\n",
    "            text = line.rstrip()\n",
    "            elem = re.split(r'\\t+', text)\n",
    "            # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸã‚‚ã®ã¯é™¤å¤–ã™ã‚‹\n",
    "            if len(elem) != 2:\n",
    "                continue\n",
    "            thread_title, num_response = elem\n",
    "            # NGãƒ•ãƒ¬ãƒ¼ã‚ºãŒå«ã¾ã‚Œã‚‹å ´åˆé™¤å¤–\n",
    "            for phrase in THREAD_FILTER_NG_PHRASES:\n",
    "                if phrase in thread_title:\n",
    "                    continue\n",
    "            # ãƒ¬ã‚¹æ•°ãŒå°‘ãªã„ã‚‚ã®ã‚’é™¤å¤–\n",
    "            if int(num_response) < THREAD_FILTER_TH_MIN_RESPONSE:\n",
    "                continue\n",
    "            titles.append(thread_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«æ•°: 2016289\n"
     ]
    }
   ],
   "source": [
    "print(f\"ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«æ•°: {len(titles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ä¾‹\n",
    "print(titles[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.normalize import ThreadTitleNormalizer\n",
    "normalizer = ThreadTitleNormalizer(IGNORE_PHRASES, SPECIAL_TOKEN_MAP, SPECIAL_TOKEN_REGEX_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<nanjmin>ã€Œå††å‘¨ç‡ã¯3<kusa>ã€ãƒ¯ã‚¤ã€ŒğŸ¤”ã€'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"å½¡(ã‚š)(ã‚š)ã€Œå††å‘¨ç‡ã¯3ï½—ï½—ï½—ã€ãƒ¯ã‚¤ã€ŒğŸ¤”ã€ [ç„¡æ–­è»¢è¼‰ç¦æ­¢]Â©2ch.net\"\n",
    "normalizer.normalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­£è¦åŒ–æ¸ˆã¿ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹\n",
    "with open(\"normalized_titles.txt\", \"w\") as f:\n",
    "    for title in titles:\n",
    "        normalized = normalizer.normalize(title)\n",
    "        if len(normalized) > 0:\n",
    "            f.write(f\"{normalized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizerã‚’Pickleã§ä¿å­˜ã™ã‚‹\n",
    "with open(\"normalizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(normalizer, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
